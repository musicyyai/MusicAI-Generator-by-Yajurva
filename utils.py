                    # utils.py (Updated for Step 6.4.5 - Suspend State Tracking)

                    import json
                    import os
                    import logging
                    import hashlib
                    import subprocess
                    import pickle
                    from google_auth_oauthlib.flow import InstalledAppFlow
                    from google.auth.transport.requests import Request
                    from googleapiclient.discovery import build
                    from googleapiclient.errors import HttpError
                    from google.auth.exceptions import RefreshError
                    from googleapiclient.http import MediaFileUpload
                    from difflib import SequenceMatcher
                    import requests
                    import socket
                    import time
                    import telegram
                    from telegram import InlineKeyboardButton, InlineKeyboardMarkup
                    import asyncio
                    import random

                    # --- Import Config and State ---
                    try:
                        from main import DEFAULT_STATE
                        from config import DRY_RUN, NUM_KAGGLE_ACCOUNTS # Import NUM_KAGGLE_ACCOUNTS here too
                    except ImportError:
                        logging.warning("Could not import from main. Using fallbacks/env vars.")
                        try:
                             from config import DRY_RUN, NUM_KAGGLE_ACCOUNTS # Try importing NUM_KAGGLE_ACCOUNTS directly
                        except ImportError:
                             logging.error("Could not import DRY_RUN/NUM_KAGGLE_ACCOUNTS from config. Using defaults.")
                             DRY_RUN = False
                             NUM_KAGGLE_ACCOUNTS = 4 # Default if config fails
                        DEFAULT_STATE = { "status": "stopped", "active_kaggle_account_index": 0, "active_drive_account_index": 0, "current_step": "idle", "current_prompt": None, "last_kaggle_run_id": None, "last_kaggle_trigger_time": None, "last_downloaded_mp3": None, "last_downloaded_json": None, "retry_count": 0, "total_tracks_generated": 0, "style_profile_id": "default", "fallback_active": False, "kaggle_usage": [{"account_index": i, "gpu_hours_used_this_week": 0.0, "last_reset_time": None, "suspended": False} for i in range(NUM_KAGGLE_ACCOUNTS)], "last_error": None, "_checksum": None, "recent_fingerprints": [], "last_gdrive_cleanup_time": None, "last_health_check_time": None, "intervention_pending_since": None }

                    DEFAULT_STYLE_PROFILE = { "profile_id": "default", "last_updated": None, "recent_bpms": [], "recent_keys": [], "genre_counts": {}, "instrument_counts": {}, "mood_counts": {}, "prompt_keyword_counts": {}, "last_reset_track_count": 0, "_checksum": None }
                    STYLE_PROFILE_FILE_PATH = "style_profile.json"

                    try:
                        import spotipy
                        from spotipy.oauth2 import SpotifyClientCredentials
                        from spotipy.exceptions import SpotifyException
                        SPOTIPY_AVAILABLE = True
                    except ImportError:
                        logging.warning("Spotipy library not found."); SPOTIPY_AVAILABLE = False; SpotifyException = None

                    # --- Resiliency Utilities ---
                    def retry_operation(func, args=None, kwargs=None, max_retries=3, delay_seconds=5, allowed_exceptions=None, operation_name="Operation"):
                        # ... (retry_operation function remains unchanged) ...
                        if args is None: args = ()
                        if kwargs is None: kwargs = {}
                        if allowed_exceptions is None:
                             allowed_exceptions = ( requests.exceptions.RequestException, socket.timeout, TimeoutError, HttpError, SpotifyException, subprocess.TimeoutExpired, )
                             allowed_exceptions = tuple(e for e in allowed_exceptions if e is not None)
                        retries = 0
                        while retries <= max_retries:
                            try:
                                logging.info(f"Attempting {operation_name} (Attempt {retries + 1}/{max_retries + 1})...")
                                result = func(*args, **kwargs)
                                logging.info(f"{operation_name} successful.")
                                return result
                            except allowed_exceptions as e:
                                logging.warning(f"{operation_name} failed on attempt {retries + 1}: {type(e).__name__} - {e}")
                                retries += 1
                                if retries <= max_retries:
                                    current_delay = delay_seconds * (2 ** (retries - 1)); jitter = current_delay * random.uniform(0.1, 0.5); wait_time = current_delay + jitter
                                    logging.info(f"Retrying {operation_name} in {wait_time:.2f} seconds...")
                                    time.sleep(wait_time)
                                else: logging.error(f"{operation_name} failed after {max_retries + 1} attempts."); logging.exception(f"Final failure details for {operation_name}:"); return None
                            except Exception as e: logging.critical(f"Unexpected error during {operation_name} (Attempt {retries + 1}): {e}", exc_info=True); return None
                        return None


                    # --- State Management Functions ---
                    def load_state(filepath): ## <<< MODIFIED >>> ##
                        logging.info(f"Attempting load state: {filepath}")
                        try:
                            if not os.path.exists(filepath): logging.warning(f"State file '{filepath}' not found."); return DEFAULT_STATE.copy();
                            with open(filepath, 'r', encoding='utf-8') as f: state_str = f.read();
                            if not state_str.strip(): logging.warning(f"State file '{filepath}' empty."); return DEFAULT_STATE.copy();
                            loaded_state = json.loads(state_str);
                            stored_checksum = loaded_state.get('_checksum');
                            if stored_checksum:
                                state_copy_for_checksum = loaded_state.copy(); state_copy_for_checksum.pop('_checksum', None);
                                try: checksum_str = json.dumps(state_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); calculated_checksum = hashlib.sha256(checksum_str).hexdigest();
                                if calculated_checksum != stored_checksum: logging.error(f"STATE CHECKSUM MISMATCH!"); return DEFAULT_STATE.copy();
                                else: logging.debug("State checksum verified.");
                                except Exception as checksum_e: logging.error(f"Failed verify checksum: {checksum_e}.");
                            else: logging.warning("No checksum in state file.");
                            state_updated = False;
                            for key, default_value in DEFAULT_STATE.items():
                                if key not in loaded_state: logging.warning(f"Key '{key}' missing."); loaded_state[key] = default_value; state_updated = True;
                            # Type checks...
                            if not isinstance(loaded_state.get("recent_fingerprints"), list): logging.warning("State 'recent_fingerprints' not list."); loaded_state["recent_fingerprints"] = []; state_updated = True
                            # Validate kaggle_usage structure
                            if not isinstance(loaded_state.get("kaggle_usage"), list):
                                 logging.warning("State 'kaggle_usage' not list. Resetting."); loaded_state["kaggle_usage"] = DEFAULT_STATE["kaggle_usage"]; state_updated = True
                            else:
                                 # Ensure list has correct number of entries and each entry has required keys
                                 # Use NUM_KAGGLE_ACCOUNTS imported from config (or default)
                                 if len(loaded_state["kaggle_usage"]) != NUM_KAGGLE_ACCOUNTS:
                                      logging.warning(f"State 'kaggle_usage' length mismatch ({len(loaded_state['kaggle_usage'])} vs {NUM_KAGGLE_ACCOUNTS}). Rebuilding.");
                                      loaded_state["kaggle_usage"] = [{"account_index": i, "gpu_hours_used_this_week": 0.0, "last_reset_time": None, "suspended": False} for i in range(NUM_KAGGLE_ACCOUNTS)]
                                      state_updated = True
                                 else:
                                      for i, usage_dict in enumerate(loaded_state["kaggle_usage"]):
                                           if not isinstance(usage_dict, dict):
                                                logging.warning(f"Item index {i} in kaggle_usage not dict. Resetting.")
                                                loaded_state["kaggle_usage"][i] = {"account_index": i, "gpu_hours_used_this_week": 0.0, "last_reset_time": None, "suspended": False}
                                                state_updated = True
                                           else:
                                                if "suspended" not in usage_dict: logging.warning(f"Adding missing 'suspended' key index {i}."); usage_dict["suspended"] = False; state_updated = True
                                                if not isinstance(usage_dict.get("gpu_hours_used_this_week"), (int, float)): logging.warning(f"Resetting non-numeric gpu_hours index {i}."); usage_dict["gpu_hours_used_this_week"] = 0.0; state_updated = True
                                                if usage_dict.get("last_reset_time") is not None and not isinstance(usage_dict.get("last_reset_time"), str): logging.warning(f"Resetting invalid last_reset_time index {i}."); usage_dict["last_reset_time"] = None; state_updated = True
                            # End kaggle_usage validation
                            if loaded_state.get("last_gdrive_cleanup_time") is not None and not isinstance(loaded_state.get("last_gdrive_cleanup_time"), str): logging.warning("State 'last_gdrive_cleanup_time' not None/string."); loaded_state["last_gdrive_cleanup_time"] = None; state_updated = True
                            if loaded_state.get("last_health_check_time") is not None and not isinstance(loaded_state.get("last_health_check_time"), str): logging.warning("State 'last_health_check_time' not None/string."); loaded_state["last_health_check_time"] = None; state_updated = True
                            if loaded_state.get("intervention_pending_since") is not None and not isinstance(loaded_state.get("intervention_pending_since"), str): logging.warning("State 'intervention_pending_since' not None/string."); loaded_state["intervention_pending_since"] = None; state_updated = True

                            if state_updated: logging.info("Loaded state updated.");
                            return loaded_state;
                        except FileNotFoundError: logging.warning(f"State file '{filepath}' not found."); return DEFAULT_STATE.copy();
                        except json.JSONDecodeError as e: logging.error(f"Failed decode state JSON: {e}."); return DEFAULT_STATE.copy();
                        except Exception as e: logging.critical(f"Unexpected error loading state: {e}", exc_info=True); return DEFAULT_STATE.copy()

                    def save_state(state_data, filepath):
                        # ... (save_state remains unchanged) ...
                        temp_filepath = filepath + ".tmp"; try: state_copy_for_checksum = state_data.copy(); state_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(state_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); state_data['_checksum'] = hashlib.sha256(checksum_str).hexdigest(); logging.debug(f"Calculated state checksum."); except Exception as checksum_e: logging.error(f"Failed calculate checksum: {checksum_e}."); state_data['_checksum'] = None; state_str = json.dumps(state_data, indent=4); with open(temp_filepath, 'w', encoding='utf-8') as f: f.write(state_str); os.replace(temp_filepath, filepath); logging.info(f"State saved to {filepath}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error saving state: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error saving state: {e}", exc_info=True); if os.path.exists(temp_filepath): try: os.remove(temp_filepath); except OSError as rm_e: logging.error(f"Failed remove temp state file: {rm_e}"); return False

                    # --- Style Profile Management Functions ---
                    def load_style_profile(filepath=STYLE_PROFILE_FILE_PATH):
                        # ... (load_style_profile remains unchanged) ...
                        logging.info(f"Attempting load style profile: {filepath}"); try: if not os.path.exists(filepath): logging.warning(f"Style profile '{filepath}' not found."); return DEFAULT_STYLE_PROFILE.copy(); with open(filepath, 'r', encoding='utf-8') as f: profile_str = f.read(); if not profile_str.strip(): logging.warning(f"Style profile '{filepath}' empty."); return DEFAULT_STYLE_PROFILE.copy(); loaded_profile = json.loads(profile_str); stored_checksum = loaded_profile.get('_checksum'); if stored_checksum: profile_copy_for_checksum = loaded_profile.copy(); profile_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(profile_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); calculated_checksum = hashlib.sha256(checksum_str).hexdigest(); if calculated_checksum != stored_checksum: logging.error(f"STYLE PROFILE CHECKSUM MISMATCH!"); return DEFAULT_STYLE_PROFILE.copy(); else: logging.debug("Style profile checksum verified."); except Exception as checksum_e: logging.error(f"Failed verify style profile checksum: {checksum_e}."); else: logging.warning("No checksum in style profile."); profile_updated = False; for key, default_value in DEFAULT_STYLE_PROFILE.items(): if key not in loaded_profile: logging.warning(f"Style profile key '{key}' missing."); loaded_profile[key] = default_value; profile_updated = True; if not isinstance(loaded_profile.get("recent_bpms"), list): logging.warning("Style 'recent_bpms' not list."); loaded_profile["recent_bpms"] = []; profile_updated = True; if not isinstance(loaded_profile.get("recent_keys"), list): logging.warning("Style 'recent_keys' not list."); loaded_profile["recent_keys"] = []; profile_updated = True; if not isinstance(loaded_profile.get("genre_counts"), dict): logging.warning("Style 'genre_counts' not dict."); loaded_profile["genre_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("instrument_counts"), dict): logging.warning("Style 'instrument_counts' not dict."); loaded_profile["instrument_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("mood_counts"), dict): logging.warning("Style 'mood_counts' not dict."); loaded_profile["mood_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("prompt_keyword_counts"), dict): logging.warning("Style 'prompt_keyword_counts' not dict."); loaded_profile["prompt_keyword_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("last_reset_track_count"), int): logging.warning("Style 'last_reset_track_count' not int."); loaded_profile["last_reset_track_count"] = 0; profile_updated = True; if profile_updated: logging.info("Loaded style profile updated."); return loaded_profile; except FileNotFoundError: logging.warning(f"Style profile '{filepath}' not found."); return DEFAULT_STYLE_PROFILE.copy(); except json.JSONDecodeError as e: logging.error(f"Failed decode style profile JSON: {e}."); return DEFAULT_STYLE_PROFILE.copy(); except Exception as e: logging.critical(f"Unexpected error loading style profile: {e}", exc_info=True); return DEFAULT_STYLE_PROFILE.copy()
                    def save_style_profile(profile_data, filepath=STYLE_PROFILE_FILE_PATH):
                        # ... (save_style_profile remains unchanged) ...
                        temp_filepath = filepath + ".tmp"; try: profile_copy_for_checksum = profile_data.copy(); profile_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(profile_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); profile_data['_checksum'] = hashlib.sha256(checksum_str).hexdigest(); logging.debug(f"Calculated style profile checksum."); except Exception as checksum_e: logging.error(f"Failed calculate style profile checksum: {checksum_e}."); profile_data['_checksum'] = None; profile_str = json.dumps(profile_data, indent=4); with open(temp_filepath, 'w', encoding='utf-8') as f: f.write(profile_str); os.replace(temp_filepath, filepath); logging.info(f"Style profile saved to {filepath}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error saving style profile: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error saving style profile: {e}", exc_info=True); if os.path.exists(temp_filepath): try: os.remove(temp_filepath); except OSError as rm_e: logging.error(f"Failed remove temp style profile file: {rm_e}"); return False

                    # --- Google Drive Authentication ---
                    # ... (authenticate_gdrive remains unchanged) ...
                    SCOPES = ['https://www.googleapis.com/auth/drive.file']; TOKEN_PICKLE_PATH = 'token.pickle'; _gdrive_service = None
                    def authenticate_gdrive(): global _gdrive_service; if _gdrive_service: return _gdrive_service; creds = None; if os.path.exists(TOKEN_PICKLE_PATH): try: with open(TOKEN_PICKLE_PATH, 'rb') as token_file: creds = pickle.load(token_file); logging.info("Loaded GDrive token."); except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e: logging.warning(f"Failed load token: {e}. Re-auth."); creds = None; except Exception as e: logging.error(f"Unexpected error loading token: {e}. Re-auth.", exc_info=True); creds = None; if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: try: logging.info("GDrive token expired. Refreshing..."); creds.refresh(Request()); logging.info("Token refreshed."); except RefreshError as e: logging.error(f"Failed refresh GDrive token (RefreshError): {e}. Re-auth."); creds = None; except requests.exceptions.RequestException as e: logging.error(f"Network error during GDrive refresh: {e}. Re-auth."); creds = None; except Exception as e: logging.error(f"Unexpected error refreshing GDrive token: {e}. Re-auth.", exc_info=True); creds = None; else: try: try: from main import GOOGLE_CREDS_INFO; except ImportError: logging.warning("Could not import GOOGLE_CREDS_INFO. Trying env."); google_creds_json_str_env = os.environ.get('GOOGLE_CREDS_JSON'); if google_creds_json_str_env: try: GOOGLE_CREDS_INFO = json.loads(google_creds_json_str_env); logging.info("Loaded GOOGLE_CREDS_INFO from env."); except json.JSONDecodeError as env_json_e: logging.critical(f"Failed parse GOOGLE_CREDS_JSON from env: {env_json_e}"); GOOGLE_CREDS_INFO = None; else: GOOGLE_CREDS_INFO = None; if not GOOGLE_CREDS_INFO: logging.critical("GDrive creds info missing."); return None; flow = InstalledAppFlow.from_client_info(GOOGLE_CREDS_INFO, SCOPES); logging.info("Attempting GDrive auth flow..."); creds = flow.run_console(); logging.info("Auth flow completed."); except Exception as e: logging.critical(f"Failed GDrive interactive auth flow: {e}", exc_info=True); return None; if creds: try: with open(TOKEN_PICKLE_PATH, 'wb') as token_file: pickle.dump(creds, token_file); logging.info(f"GDrive token saved."); except (IOError, OSError) as e: logging.error(f"Failed save GDrive token pickle: {e}"); except Exception as e: logging.error(f"Unexpected error saving token pickle: {e}", exc_info=True); else: logging.error("Auth resulted in invalid GDrive creds."); return None; if not creds: logging.error("Cannot build GDrive service: No valid creds."); return None; try: _gdrive_service = build('drive', 'v3', credentials=creds); logging.info("GDrive service built."); return _gdrive_service; except HttpError as e: logging.critical(f"Failed build GDrive service (HttpError): {e}", exc_info=True); return None; except Exception as e: logging.critical(f"Unexpected error building GDrive service: {e}", exc_info=True); return None

                    # --- Google Drive File Operations ---
                    # ... (upload_to_gdrive, get_gdrive_files, delete_gdrive_file remain unchanged) ...
                    def upload_to_gdrive(service, local_filepath, gdrive_folder_id, gdrive_filename): if DRY_RUN: logging.warning(f"[DRY RUN] Skipping GDrive upload of '{local_filepath}' as '{gdrive_filename}'"); return f"dry_run_fake_id_{random.randint(1000,9999)}"; if not service: logging.error("GDrive service invalid."); return None; try: if not os.path.exists(local_filepath): logging.error(f"Local file '{local_filepath}' not found."); return None; logging.info(f"Uploading '{local_filepath}' to Drive as '{gdrive_filename}'..."); file_metadata = {'name': gdrive_filename, 'parents': [gdrive_folder_id]}; media = MediaFileUpload(local_filepath, resumable=True); file = service.files().create(body=file_metadata, media_body=media, fields='id').execute(); uploaded_file_id = file.get('id'); logging.info(f"File uploaded. ID: {uploaded_file_id}"); return uploaded_file_id; except HttpError as e: logging.error(f"Google API HTTP error during upload: {e}", exc_info=True); if e.resp.status == 403 and 'quota' in str(e).lower(): logging.critical("Google Drive Quota Exceeded!"); return None; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error during GDrive upload: {e}", exc_info=True); return None; except requests.exceptions.RequestException as e: logging.error(f"Network error during GDrive upload: {e}", exc_info=True); return None; except FileNotFoundError: logging.error(f"Local file '{local_filepath}' unavailable during upload."); return None; except Exception as e: logging.critical(f"Unexpected error during GDrive upload: {e}", exc_info=True); return None
                    def get_gdrive_files(service, folder_id): if not service: logging.error("GDrive service invalid."); return []; files_list = []; page_token = None; try: logging.info(f"Listing files in GDrive folder: {folder_id}"); while True: response = service.files().list(q=f"'{folder_id}' in parents and trashed=false", spaces='drive', fields='nextPageToken, files(id, name, createdTime)', pageSize=100, pageToken=page_token).execute(); files = response.get('files', []); if not files and page_token is None and not files_list: logging.info("No files found."); break; files_list.extend(files); page_token = response.get('nextPageToken', None); if page_token is None: break; logging.info(f"Found {len(files_list)} total files."); return files_list; except HttpError as e: logging.error(f"Google API HTTP error listing files: {e}", exc_info=True); return []; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error listing files: {e}", exc_info=True); return []; except requests.exceptions.RequestException as e: logging.error(f"Network error listing files: {e}", exc_info=True); return []; except Exception as e: logging.critical(f"Unexpected error listing files: {e}", exc_info=True); return []
                    def delete_gdrive_file(service, file_id): if DRY_RUN: logging.warning(f"[DRY RUN] Skipping GDrive deletion of file ID: {file_id}"); return True; if not service: logging.error("GDrive service invalid."); return False; if not file_id: logging.error("No file ID provided."); return False; try: logging.warning(f"Attempting delete GDrive file ID: {file_id}"); service.files().delete(fileId=file_id).execute(); logging.info(f"Deleted GDrive file ID: {file_id}"); return True; except HttpError as e: if e.resp.status == 404: logging.warning(f"File ID {file_id} not found."); return True; elif e.resp.status == 403: logging.error(f"Permission error deleting {file_id}: {e}"); return False; else: logging.error(f"Google API HTTP error deleting {file_id}: {e}", exc_info=True); return False; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error deleting {file_id}: {e}", exc_info=True); return False; except requests.exceptions.RequestException as e: logging.error(f"Network error deleting {file_id}: {e}", exc_info=True); return False; except Exception as e: logging.critical(f"Unexpected error deleting {file_id}: {e}", exc_info=True); return False

                    # --- Kaggle API Setup ---
                    # ... (setup_kaggle_api remains unchanged) ...
                    KAGGLE_CONFIG_DIR = os.path.expanduser("~/.kaggle"); KAGGLE_JSON_PATH = os.path.join(KAGGLE_CONFIG_DIR, "kaggle.json")
                    def setup_kaggle_api(account_index): logging.info(f"Setting up Kaggle API index: {account_index}"); try: try: from main import KAGGLE_CREDENTIALS_LIST; except ImportError: logging.warning("Could not import KAGGLE_CREDENTIALS_LIST. Trying env."); KAGGLE_CREDENTIALS_LIST = [os.environ.get(f'KAGGLE_JSON_{i+1}') for i in range(NUM_KAGGLE_ACCOUNTS)]; if not any(KAGGLE_CREDENTIALS_LIST): logging.critical("Kaggle creds missing from env."); return False; if None in KAGGLE_CREDENTIALS_LIST: logging.warning("Some Kaggle creds missing from env."); logging.info("Loaded KAGGLE_CREDENTIALS_LIST from env."); except Exception as import_e: logging.critical(f"Failed load KAGGLE_CREDENTIALS_LIST: {import_e}"); return False; num_creds = len(KAGGLE_CREDENTIALS_LIST); if not 0 <= account_index < num_creds: logging.error(f"Invalid Kaggle index: {account_index} (of {num_creds})."); return False; kaggle_json_str = KAGGLE_CREDENTIALS_LIST[account_index]; if not kaggle_json_str: logging.error(f"Kaggle creds JSON missing index {account_index}."); return False; try: os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True); with open(KAGGLE_JSON_PATH, 'w') as f: f.write(kaggle_json_str); os.chmod(KAGGLE_JSON_PATH, 0o600); logging.info(f"Kaggle API setup ok index {account_index}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error Kaggle setup: {e}", exc_info=True); return False; except Exception as e: logging.critical(f"Unexpected error Kaggle setup: {e}", exc_info=True); return False

                    # --- Kaggle Notebook Execution ---
                    # ... (trigger_kaggle_notebook, check_kaggle_status, download_kaggle_output remain unchanged) ...
                    PARAMS_JSON_FILENAME = "params.json"; PARAMS_DATASET_SLUG = "notebook-params-temp"
                    def trigger_kaggle_notebook(notebook_slug, params_dict): if DRY_RUN: logging.warning(f"[DRY RUN] Skipping Kaggle trigger for {notebook_slug}"); return True; logging.info(f"Triggering Kaggle notebook: {notebook_slug}"); try: params_json_str = json.dumps(params_dict); except TypeError as e: logging.error(f"Failed serialize params: {e}"); return False; try: with open(PARAMS_JSON_FILENAME, 'w') as f: f.write(params_json_str); logging.info(f"Created {PARAMS_JSON_FILENAME}"); except (IOError, OSError) as e: logging.error(f"Failed write params file: {e}"); return False; metadata_content = {"title": "Notebook Params Temp", "id": f"{notebook_slug.split('/')[0]}/{PARAMS_DATASET_SLUG}", "licenses": [{"name": "CC0-1.0"}]}; metadata_filename = "dataset-metadata.json"; dataset_created = False; try: with open(metadata_filename, 'w') as f: json.dump(metadata_content, f, indent=4); logging.info(f"Created {metadata_filename}"); logging.info(f"Uploading {PARAMS_JSON_FILENAME} as dataset..."); command = ["kaggle", "datasets", "create", "-p", ".", "-m", "Update params", "--dir-mode", "skip"]; result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=120); if result.stdout: logging.info(f"Kaggle ds create stdout:\n{result.stdout}"); if result.stderr: logging.warning(f"Kaggle ds create stderr:\n{result.stderr}"); if result.returncode != 0 or ("error" in result.stderr.lower() and "error updating dataset" not in result.stderr.lower()): logging.error(f"Kaggle ds create/update failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); dataset_created = False; else: logging.info("Kaggle dataset created/updated."); dataset_created = True; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); dataset_created = False; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle dataset creation."); dataset_created = False; except (IOError, OSError) as e: logging.error(f"File I/O error dataset metadata: {e}"); dataset_created = False; except Exception as e: logging.critical(f"Unexpected error Kaggle dataset creation: {e}", exc_info=True); dataset_created = False; finally: if os.path.exists(PARAMS_JSON_FILENAME): try: os.remove(PARAMS_JSON_FILENAME); except OSError: pass; if os.path.exists(metadata_filename): try: os.remove(metadata_filename); except OSError: pass; if not dataset_created: return False; try: logging.info(f"Triggering Kaggle kernel push: {notebook_slug}"); params_dataset_full_slug = f"{notebook_slug.split('/')[0]}/{PARAMS_DATASET_SLUG}"; dummy_dir = "kaggle_push_dummy"; os.makedirs(dummy_dir, exist_ok=True); kernel_metadata = {"id": notebook_slug, "language": "python", "kernel_type": "notebook", "is_private": "true", "enable_gpu": "true", "enable_internet": "true", "dataset_sources": [params_dataset_full_slug], "competition_sources": [], "kernel_sources": []}; kernel_metadata_path = os.path.join(dummy_dir, "kernel-metadata.json"); with open(kernel_metadata_path, 'w') as f: json.dump(kernel_metadata, f); logging.info(f"Pushing kernel {notebook_slug}..."); command_push = ["kaggle", "kernels", "push", "-p", dummy_dir]; result_push = subprocess.run(command_push, capture_output=True, text=True, check=False, timeout=120); if result_push.stdout: logging.info(f"Kaggle push stdout:\n{result_push.stdout}"); if result_push.stderr: logging.warning(f"Kaggle push stderr:\n{result_push.stderr}"); if result_push.returncode == 0 and "successfully" in result_push.stdout.lower(): logging.info("Kaggle kernel push initiated."); return True; else: logging.error(f"Kaggle push failed/no success msg. Code: {result_push.returncode}."); return False; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return False; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle kernel push."); return False; except (IOError, OSError) as e: logging.error(f"File I/O error kernel push setup: {e}"); return False; except Exception as e: logging.critical(f"Unexpected error Kaggle kernel push: {e}", exc_info=True); return False; finally: if 'dummy_dir' in locals() and os.path.exists(dummy_dir): try: if os.path.exists(kernel_metadata_path): os.remove(kernel_metadata_path); os.rmdir(dummy_dir); except OSError as e: logging.warning(f"Could not cleanup dummy push dir: {e}")
                    def check_kaggle_status(notebook_slug): logging.debug(f"Checking Kaggle status: {notebook_slug}"); command = ["kaggle", "kernels", "status", notebook_slug]; try: result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=60); output_line = result.stdout.strip(); if not output_line and result.stderr and "status" in result.stderr.lower(): output_line = result.stderr.strip().split('\n')[-1]; if result.returncode != 0: logging.error(f"Kaggle status cmd failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); if "401" in result.stderr: logging.error("Kaggle API auth error (401)."); elif "404" in result.stderr: logging.error("Kaggle kernel not found (404)."); elif "429" in result.stderr: logging.warning("Kaggle API rate limit (429)."); return None; if not output_line: logging.warning("Kaggle status empty output."); return None; status_part = None; if ":" in output_line: status_part = output_line.split(':')[-1].strip().lower(); elif "-" in output_line: status_part = output_line.split('-')[-1].strip().lower(); if status_part: if "error" in status_part: return "error"; if "complete" in status_part: return "complete"; if "running" in status_part: return "running"; if "cancelled" in status_part: return "cancelled"; if "queued" in status_part: return "queued"; logging.warning(f"Unknown status parsed: '{status_part}'"); return None; else: logging.warning(f"Could not parse status line: '{output_line}'"); return None; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return None; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle status check."); return None; except Exception as e: logging.critical(f"Unexpected error Kaggle status check: {e}", exc_info=True); return None
                    def download_kaggle_output(notebook_slug, destination_dir=".", download_image=False): logging.info(f"Attempting download from: {notebook_slug}"); try: os.makedirs(destination_dir, exist_ok=True); except OSError as e: logging.error(f"Failed create dest dir: {e}"); return None, None, None; command = ["kaggle", "kernels", "output", notebook_slug, "-p", destination_dir, "--force"]; mp3_path, json_path, img_path = None, None, None; try: logging.info(f"Running Kaggle download..."); result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=300); if result.stdout: logging.info(f"Kaggle download stdout:\n{result.stdout}"); if result.stderr: logging.warning(f"Kaggle download stderr:\n{result.stderr}"); if result.returncode != 0: stderr_lower = result.stderr.lower(); if "404" in stderr_lower or "not found" in stderr_lower: logging.error("Kaggle download failed: 404."); elif "no output files" in stderr_lower: logging.warning("Kaggle download: No output files."); elif "401" in stderr_lower: logging.error("Kaggle download failed: 401."); elif "429" in stderr_lower: logging.warning("Kaggle download failed: 429."); else: logging.error(f"Kaggle download cmd failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); return None, None, None; logging.info("Kaggle download finished. Verifying..."); potential_mp3_path = os.path.join(destination_dir, KAGGLE_OUTPUT_MP3); if os.path.exists(potential_mp3_path) and os.path.getsize(potential_mp3_path) > 100: mp3_path = potential_mp3_path; logging.info(f"Verified MP3: {mp3_path}"); else: logging.warning(f"MP3 '{KAGGLE_OUTPUT_MP3}' missing/empty."); potential_json_path = os.path.join(destination_dir, KAGGLE_OUTPUT_JSON); if os.path.exists(potential_json_path) and os.path.getsize(potential_json_path) > 2: json_path = potential_json_path; logging.info(f"Verified JSON: {json_path}"); else: logging.warning(f"JSON '{KAGGLE_OUTPUT_JSON}' missing/empty."); if download_image: potential_img_path = os.path.join(destination_dir, KAGGLE_OUTPUT_IMG); if os.path.exists(potential_img_path) and os.path.getsize(potential_img_path) > 100: img_path = potential_img_path; logging.info(f"Verified Image: {img_path}"); else: logging.warning(f"Image '{KAGGLE_OUTPUT_IMG}' missing/empty."); return mp3_path, json_path, img_path; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return None, None, None; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle download."); return None, None, None; except Exception as e: logging.critical(f"Unexpected error Kaggle download: {e}", exc_info=True); return None, None, None

                    # --- Uniqueness Check ---
                    # ... (compare_fingerprints, is_unique_enough remain unchanged) ...
                    def compare_fingerprints(fp1_str, fp2_str): if not fp1_str or not fp2_str: return 0.0; return SequenceMatcher(None, fp1_str, fp2_str).ratio()
                    def is_unique_enough(new_fingerprint, recent_fingerprints, threshold): if not new_fingerprint: logging.warning("New fingerprint empty."); return False; if not recent_fingerprints: return True; max_similarity = 0.0; for old_fingerprint in recent_fingerprints: similarity = compare_fingerprints(new_fingerprint, old_fingerprint); logging.debug(f"Comparing fingerprints: Sim = {similarity:.4f}"); if similarity > max_similarity: max_similarity = similarity; if similarity > threshold: logging.warning(f"Track too similar (Sim: {similarity:.4f} > Thr: {threshold})."); return False; logging.info(f"Uniqueness check passed. Max sim: {max_similarity:.4f} (Thr: {threshold})"); return True

                    # --- Spotify API Interaction ---
                    # ... (get_spotify_client, get_spotify_trending_keywords remain unchanged) ...
                    _spotify_client = None
                    def get_spotify_client(): global _spotify_client; if _spotify_client: return _spotify_client; if not SPOTIPY_AVAILABLE: logging.error("Spotipy not installed."); return None; logging.info("Authenticating with Spotify..."); try: client_id=os.environ.get('SPOTIPY_CLIENT_ID'); client_secret=os.environ.get('SPOTIPY_CLIENT_SECRET'); if not client_id or not client_secret: logging.error("Spotify creds missing."); return None; client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret); sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager); sp.categories(limit=1); logging.info("Spotify client authenticated."); _spotify_client = sp; return _spotify_client; except SpotifyException as e: logging.error(f"Spotify API auth/call error: {e}", exc_info=True); return None; except requests.exceptions.RequestException as e: logging.error(f"Network error Spotify auth: {e}", exc_info=True); return None; except Exception as e: logging.critical(f"Unexpected error Spotify auth: {e}", exc_info=True); return None
                    def get_spotify_trending_keywords(limit=10): sp = get_spotify_client(); if not sp: logging.error("Cannot fetch Spotify keywords: client unavailable."); return []; keywords = set(); final_keywords = []; try: logging.info("Fetching Spotify categories..."); categories = sp.categories(country='US', limit=max(limit // 2, 5))['categories']['items']; for category in categories: keywords.add(category['name'].lower()); logging.info("Fetching Spotify new releases..."); new_releases = sp.new_releases(limit=limit)['albums']['items']; for album in new_releases: for artist in album['artists']: try: artist_info = sp.artist(artist['id']); keywords.update(g.lower() for g in artist_info.get('genres', [])); except SpotifyException as art_e: logging.warning(f"Spotify API error artist genres {artist.get('name', artist['id'])}: {art_e}"); except requests.exceptions.RequestException as net_e: logging.warning(f"Network error artist genres {artist.get('name', artist['id'])}: {net_e}"); except Exception as e: logging.warning(f"Unexpected error artist genres {artist.get('name', artist['id'])}: {e}"); final_keywords = list(keywords)[:limit]; logging.info(f"Fetched {len(final_keywords)} Spotify keywords."); except SpotifyException as e: logging.error(f"Spotify API error fetching keywords: {e}", exc_info=True); except requests.exceptions.RequestException as e: logging.error(f"Network error fetching keywords: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error fetching keywords: {e}", exc_info=True); return final_keywords

                    # --- Telegram Notification Utility ---
                    # ... (send_telegram_message remains unchanged) ...
                    _telegram_bot_instance = None
                    def _get_telegram_bot(): global _telegram_bot_instance; if _telegram_bot_instance is None: token = os.environ.get('TELEGRAM_BOT_TOKEN'); if not token: logging.error("Cannot init Telegram Bot: Token secret missing."); return None; try: _telegram_bot_instance = telegram.Bot(token=token); logging.info("Telegram Bot instance initialized."); except Exception as e: logging.error(f"Failed init Telegram Bot: {e}", exc_info=True); _telegram_bot_instance = None; return _telegram_bot_instance
                    async def _send_message_async(bot, chat_id, text, reply_markup=None): try: await bot.send_message(chat_id=chat_id, text=text, parse_mode='Markdown', reply_markup=reply_markup); logging.info(f"Sent Telegram message to chat_id {chat_id}."); return True; except telegram.error.TelegramError as e: logging.error(f"Telegram API error sending message: {e}", exc_info=True); return False; except Exception as e: logging.error(f"Unexpected error sending Telegram message: {e}", exc_info=True); return False
                    def send_telegram_message(message, level="INFO", reply_markup=None): bot = _get_telegram_bot(); chat_id = os.environ.get('TELEGRAM_CHAT_ID'); if not bot: logging.error("TG message not sent: Bot not initialized."); return False; if not chat_id: logging.error("TG message not sent: Chat ID secret missing."); return False; prefix = f"[{level.upper()}]"; full_message = f"{prefix} {message}"; max_len = 4090; if len(full_message) > max_len: logging.warning(f"TG message too long. Truncating."); full_message = full_message[:max_len] + "..."; logging.info(f"Attempting send Telegram message (Level: {level})..."); try: success = asyncio.run(_send_message_async(bot, chat_id, full_message, reply_markup=reply_markup)); return success; except RuntimeError as e: logging.error(f"RuntimeError calling asyncio.run for Telegram: {e}. Loop running?"); return False; except Exception as e: logging.error(f"Unexpected error setting up asyncio for Telegram send: {e}", exc_info=True); return False