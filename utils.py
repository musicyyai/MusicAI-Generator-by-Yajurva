            # utils.py (Updated for Step 6.6.1 - State Tracking)

            import json
            import os
            import logging
            import hashlib
            import subprocess
            import pickle
            from google_auth_oauthlib.flow import InstalledAppFlow
            from google.auth.transport.requests import Request
            from googleapiclient.discovery import build
            from googleapiclient.errors import HttpError
            from google.auth.exceptions import RefreshError
            from googleapiclient.http import MediaFileUpload
            from difflib import SequenceMatcher
            import requests
            import socket
            import time
            import telegram
            import asyncio
            import random # Ensure random is imported for retry jitter

            try:
                from main import DEFAULT_STATE
            except ImportError:
                logging.warning("Could not import DEFAULT_STATE from main. Using fallback."); DEFAULT_STATE = { "status": "stopped", "active_kaggle_account_index": 0, "active_drive_account_index": 0, "current_step": "idle", "current_prompt": None, "last_kaggle_run_id": None, "last_kaggle_trigger_time": None, "last_downloaded_mp3": None, "last_downloaded_json": None, "retry_count": 0, "total_tracks_generated": 0, "style_profile_id": "default", "fallback_active": False, "kaggle_usage": [{"account_index": i, "gpu_hours_used_this_week": 0.0, "last_reset_time": None} for i in range(4)], "last_error": None, "_checksum": None, "recent_fingerprints": [], "last_gdrive_cleanup_time": None, "last_health_check_time": None } # Added last_health_check_time

            DEFAULT_STYLE_PROFILE = { "profile_id": "default", "last_updated": None, "recent_bpms": [], "recent_keys": [], "genre_counts": {}, "instrument_counts": {}, "mood_counts": {}, "prompt_keyword_counts": {}, "last_reset_track_count": 0, "_checksum": None }
            STYLE_PROFILE_FILE_PATH = "style_profile.json"

            try:
                import spotipy
                from spotipy.oauth2 import SpotifyClientCredentials
                from spotipy.exceptions import SpotifyException
                SPOTIPY_AVAILABLE = True
            except ImportError:
                logging.warning("Spotipy library not found."); SPOTIPY_AVAILABLE = False; SpotifyException = None

            # --- Resiliency Utilities ---
            def retry_operation(func, args=None, kwargs=None, max_retries=3, delay_seconds=5, allowed_exceptions=None, operation_name="Operation"):
                # ... (retry_operation function remains unchanged) ...
                if args is None: args = ()
                if kwargs is None: kwargs = {}
                if allowed_exceptions is None:
                     allowed_exceptions = ( requests.exceptions.RequestException, socket.timeout, TimeoutError, HttpError, SpotifyException, subprocess.TimeoutExpired, )
                     allowed_exceptions = tuple(e for e in allowed_exceptions if e is not None)
                retries = 0
                while retries <= max_retries:
                    try:
                        logging.info(f"Attempting {operation_name} (Attempt {retries + 1}/{max_retries + 1})...")
                        result = func(*args, **kwargs)
                        logging.info(f"{operation_name} successful.")
                        return result
                    except allowed_exceptions as e:
                        logging.warning(f"{operation_name} failed on attempt {retries + 1}: {type(e).__name__} - {e}")
                        retries += 1
                        if retries <= max_retries:
                            current_delay = delay_seconds * (2 ** (retries - 1)); jitter = current_delay * random.uniform(0.1, 0.5); wait_time = current_delay + jitter
                            logging.info(f"Retrying {operation_name} in {wait_time:.2f} seconds...")
                            time.sleep(wait_time)
                        else: logging.error(f"{operation_name} failed after {max_retries + 1} attempts."); logging.exception(f"Final failure details for {operation_name}:"); return None
                    except Exception as e: logging.critical(f"Unexpected error during {operation_name} (Attempt {retries + 1}): {e}", exc_info=True); return None
                return None


            # --- State Management Functions ---
            def load_state(filepath):
                logging.info(f"Attempting load state: {filepath}")
                try:
                    # ... (rest of load_state unchanged until type checks) ...
                    if not os.path.exists(filepath): logging.warning(f"State file '{filepath}' not found."); return DEFAULT_STATE.copy();
                    with open(filepath, 'r', encoding='utf-8') as f: state_str = f.read();
                    if not state_str.strip(): logging.warning(f"State file '{filepath}' empty."); return DEFAULT_STATE.copy();
                    loaded_state = json.loads(state_str);
                    stored_checksum = loaded_state.get('_checksum');
                    if stored_checksum:
                        state_copy_for_checksum = loaded_state.copy(); state_copy_for_checksum.pop('_checksum', None);
                        try: checksum_str = json.dumps(state_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); calculated_checksum = hashlib.sha256(checksum_str).hexdigest();
                        if calculated_checksum != stored_checksum: logging.error(f"STATE CHECKSUM MISMATCH!"); return DEFAULT_STATE.copy();
                        else: logging.debug("State checksum verified.");
                        except Exception as checksum_e: logging.error(f"Failed verify checksum: {checksum_e}.");
                    else: logging.warning("No checksum in state file.");
                    state_updated = False;
                    for key, default_value in DEFAULT_STATE.items():
                        if key not in loaded_state: logging.warning(f"Key '{key}' missing."); loaded_state[key] = default_value; state_updated = True;
                    # Type checks... ## <<< MODIFIED >>> ##
                    if not isinstance(loaded_state.get("recent_fingerprints"), list): logging.warning("State 'recent_fingerprints' not list."); loaded_state["recent_fingerprints"] = []; state_updated = True
                    if not isinstance(loaded_state.get("kaggle_usage"), list): logging.warning("State 'kaggle_usage' not list."); loaded_state["kaggle_usage"] = DEFAULT_STATE["kaggle_usage"]; state_updated = True
                    if loaded_state.get("last_gdrive_cleanup_time") is not None and not isinstance(loaded_state.get("last_gdrive_cleanup_time"), str): logging.warning("State 'last_gdrive_cleanup_time' not None/string."); loaded_state["last_gdrive_cleanup_time"] = None; state_updated = True
                    if loaded_state.get("last_health_check_time") is not None and not isinstance(loaded_state.get("last_health_check_time"), str): logging.warning("State 'last_health_check_time' not None/string."); loaded_state["last_health_check_time"] = None; state_updated = True # <<< ADDED TYPE CHECK

                    if state_updated: logging.info("Loaded state updated.");
                    return loaded_state;
                except FileNotFoundError: logging.warning(f"State file '{filepath}' not found."); return DEFAULT_STATE.copy();
                except json.JSONDecodeError as e: logging.error(f"Failed decode state JSON: {e}."); return DEFAULT_STATE.copy();
                except Exception as e: logging.critical(f"Unexpected error loading state: {e}", exc_info=True); return DEFAULT_STATE.copy()

            def save_state(state_data, filepath):
                # ... (save_state remains unchanged) ...
                temp_filepath = filepath + ".tmp"; try: state_copy_for_checksum = state_data.copy(); state_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(state_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); state_data['_checksum'] = hashlib.sha256(checksum_str).hexdigest(); logging.debug(f"Calculated state checksum."); except Exception as checksum_e: logging.error(f"Failed calculate checksum: {checksum_e}."); state_data['_checksum'] = None; state_str = json.dumps(state_data, indent=4); with open(temp_filepath, 'w', encoding='utf-8') as f: f.write(state_str); os.replace(temp_filepath, filepath); logging.info(f"State saved to {filepath}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error saving state: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error saving state: {e}", exc_info=True); if os.path.exists(temp_filepath): try: os.remove(temp_filepath); except OSError as rm_e: logging.error(f"Failed remove temp state file: {rm_e}"); return False

            # --- Style Profile Management Functions ---
            # ... (load_style_profile, save_style_profile remain unchanged) ...
            def load_style_profile(filepath=STYLE_PROFILE_FILE_PATH): logging.info(f"Attempting load style profile: {filepath}"); try: if not os.path.exists(filepath): logging.warning(f"Style profile '{filepath}' not found."); return DEFAULT_STYLE_PROFILE.copy(); with open(filepath, 'r', encoding='utf-8') as f: profile_str = f.read(); if not profile_str.strip(): logging.warning(f"Style profile '{filepath}' empty."); return DEFAULT_STYLE_PROFILE.copy(); loaded_profile = json.loads(profile_str); stored_checksum = loaded_profile.get('_checksum'); if stored_checksum: profile_copy_for_checksum = loaded_profile.copy(); profile_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(profile_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); calculated_checksum = hashlib.sha256(checksum_str).hexdigest(); if calculated_checksum != stored_checksum: logging.error(f"STYLE PROFILE CHECKSUM MISMATCH!"); return DEFAULT_STYLE_PROFILE.copy(); else: logging.debug("Style profile checksum verified."); except Exception as checksum_e: logging.error(f"Failed verify style profile checksum: {checksum_e}."); else: logging.warning("No checksum in style profile."); profile_updated = False; for key, default_value in DEFAULT_STYLE_PROFILE.items(): if key not in loaded_profile: logging.warning(f"Style profile key '{key}' missing."); loaded_profile[key] = default_value; profile_updated = True; if not isinstance(loaded_profile.get("recent_bpms"), list): logging.warning("Style 'recent_bpms' not list."); loaded_profile["recent_bpms"] = []; profile_updated = True; if not isinstance(loaded_profile.get("recent_keys"), list): logging.warning("Style 'recent_keys' not list."); loaded_profile["recent_keys"] = []; profile_updated = True; if not isinstance(loaded_profile.get("genre_counts"), dict): logging.warning("Style 'genre_counts' not dict."); loaded_profile["genre_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("instrument_counts"), dict): logging.warning("Style 'instrument_counts' not dict."); loaded_profile["instrument_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("mood_counts"), dict): logging.warning("Style 'mood_counts' not dict."); loaded_profile["mood_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("prompt_keyword_counts"), dict): logging.warning("Style 'prompt_keyword_counts' not dict."); loaded_profile["prompt_keyword_counts"] = {}; profile_updated = True; if not isinstance(loaded_profile.get("last_reset_track_count"), int): logging.warning("Style 'last_reset_track_count' not int."); loaded_profile["last_reset_track_count"] = 0; profile_updated = True; if profile_updated: logging.info("Loaded style profile updated."); return loaded_profile; except FileNotFoundError: logging.warning(f"Style profile '{filepath}' not found."); return DEFAULT_STYLE_PROFILE.copy(); except json.JSONDecodeError as e: logging.error(f"Failed decode style profile JSON: {e}."); return DEFAULT_STYLE_PROFILE.copy(); except Exception as e: logging.critical(f"Unexpected error loading style profile: {e}", exc_info=True); return DEFAULT_STYLE_PROFILE.copy()
            def save_style_profile(profile_data, filepath=STYLE_PROFILE_FILE_PATH): temp_filepath = filepath + ".tmp"; try: profile_copy_for_checksum = profile_data.copy(); profile_copy_for_checksum.pop('_checksum', None); try: checksum_str = json.dumps(profile_copy_for_checksum, separators=(',', ':'), sort_keys=True).encode('utf-8'); profile_data['_checksum'] = hashlib.sha256(checksum_str).hexdigest(); logging.debug(f"Calculated style profile checksum."); except Exception as checksum_e: logging.error(f"Failed calculate style profile checksum: {checksum_e}."); profile_data['_checksum'] = None; profile_str = json.dumps(profile_data, indent=4); with open(temp_filepath, 'w', encoding='utf-8') as f: f.write(profile_str); os.replace(temp_filepath, filepath); logging.info(f"Style profile saved to {filepath}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error saving style profile: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error saving style profile: {e}", exc_info=True); if os.path.exists(temp_filepath): try: os.remove(temp_filepath); except OSError as rm_e: logging.error(f"Failed remove temp style profile file: {rm_e}"); return False

            # --- Google Drive Authentication ---
            # ... (authenticate_gdrive remains unchanged) ...
            SCOPES = ['https://www.googleapis.com/auth/drive.file']; TOKEN_PICKLE_PATH = 'token.pickle'; _gdrive_service = None
            def authenticate_gdrive(): global _gdrive_service; if _gdrive_service: return _gdrive_service; creds = None; if os.path.exists(TOKEN_PICKLE_PATH): try: with open(TOKEN_PICKLE_PATH, 'rb') as token_file: creds = pickle.load(token_file); logging.info("Loaded GDrive token."); except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e: logging.warning(f"Failed load token: {e}. Re-auth."); creds = None; except Exception as e: logging.error(f"Unexpected error loading token: {e}. Re-auth.", exc_info=True); creds = None; if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: try: logging.info("GDrive token expired. Refreshing..."); creds.refresh(Request()); logging.info("Token refreshed."); except RefreshError as e: logging.error(f"Failed refresh GDrive token (RefreshError): {e}. Re-auth."); creds = None; except requests.exceptions.RequestException as e: logging.error(f"Network error during GDrive refresh: {e}. Re-auth."); creds = None; except Exception as e: logging.error(f"Unexpected error refreshing GDrive token: {e}. Re-auth.", exc_info=True); creds = None; else: try: try: from main import GOOGLE_CREDS_INFO; except ImportError: logging.warning("Could not import GOOGLE_CREDS_INFO. Trying env."); google_creds_json_str_env = os.environ.get('GOOGLE_CREDS_JSON'); if google_creds_json_str_env: try: GOOGLE_CREDS_INFO = json.loads(google_creds_json_str_env); logging.info("Loaded GOOGLE_CREDS_INFO from env."); except json.JSONDecodeError as env_json_e: logging.critical(f"Failed parse GOOGLE_CREDS_JSON from env: {env_json_e}"); GOOGLE_CREDS_INFO = None; else: GOOGLE_CREDS_INFO = None; if not GOOGLE_CREDS_INFO: logging.critical("GDrive creds info missing."); return None; flow = InstalledAppFlow.from_client_info(GOOGLE_CREDS_INFO, SCOPES); logging.info("Attempting GDrive auth flow..."); creds = flow.run_console(); logging.info("Auth flow completed."); except Exception as e: logging.critical(f"Failed GDrive interactive auth flow: {e}", exc_info=True); return None; if creds: try: with open(TOKEN_PICKLE_PATH, 'wb') as token_file: pickle.dump(creds, token_file); logging.info(f"GDrive token saved."); except (IOError, OSError) as e: logging.error(f"Failed save GDrive token pickle: {e}"); except Exception as e: logging.error(f"Unexpected error saving token pickle: {e}", exc_info=True); else: logging.error("Auth resulted in invalid GDrive creds."); return None; if not creds: logging.error("Cannot build GDrive service: No valid creds."); return None; try: _gdrive_service = build('drive', 'v3', credentials=creds); logging.info("GDrive service built."); return _gdrive_service; except HttpError as e: logging.critical(f"Failed build GDrive service (HttpError): {e}", exc_info=True); return None; except Exception as e: logging.critical(f"Unexpected error building GDrive service: {e}", exc_info=True); return None

            # --- Google Drive File Operations ---
            # ... (upload_to_gdrive, get_gdrive_files, delete_gdrive_file remain unchanged) ...
            def upload_to_gdrive(service, local_filepath, gdrive_folder_id, gdrive_filename): if not service: logging.error("GDrive service invalid."); return None; try: if not os.path.exists(local_filepath): logging.error(f"Local file '{local_filepath}' not found."); return None; logging.info(f"Uploading '{local_filepath}' to Drive as '{gdrive_filename}'..."); file_metadata = {'name': gdrive_filename, 'parents': [gdrive_folder_id]}; media = MediaFileUpload(local_filepath, resumable=True); file = service.files().create(body=file_metadata, media_body=media, fields='id').execute(); uploaded_file_id = file.get('id'); logging.info(f"File uploaded. ID: {uploaded_file_id}"); return uploaded_file_id; except HttpError as e: logging.error(f"Google API HTTP error during upload: {e}", exc_info=True); if e.resp.status == 403 and 'quota' in str(e).lower(): logging.critical("Google Drive Quota Exceeded!"); return None; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error during GDrive upload: {e}", exc_info=True); return None; except requests.exceptions.RequestException as e: logging.error(f"Network error during GDrive upload: {e}", exc_info=True); return None; except FileNotFoundError: logging.error(f"Local file '{local_filepath}' unavailable during upload."); return None; except Exception as e: logging.critical(f"Unexpected error during GDrive upload: {e}", exc_info=True); return None
            def get_gdrive_files(service, folder_id): if not service: logging.error("GDrive service invalid."); return []; files_list = []; page_token = None; try: logging.info(f"Listing files in GDrive folder: {folder_id}"); while True: response = service.files().list(q=f"'{folder_id}' in parents and trashed=false", spaces='drive', fields='nextPageToken, files(id, name, createdTime)', pageSize=100, pageToken=page_token).execute(); files = response.get('files', []); if not files and page_token is None and not files_list: logging.info("No files found."); break; files_list.extend(files); page_token = response.get('nextPageToken', None); if page_token is None: break; logging.info(f"Found {len(files_list)} total files."); return files_list; except HttpError as e: logging.error(f"Google API HTTP error listing files: {e}", exc_info=True); return []; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error listing files: {e}", exc_info=True); return []; except requests.exceptions.RequestException as e: logging.error(f"Network error listing files: {e}", exc_info=True); return []; except Exception as e: logging.critical(f"Unexpected error listing files: {e}", exc_info=True); return []
            def delete_gdrive_file(service, file_id): if not service: logging.error("GDrive service invalid."); return False; if not file_id: logging.error("No file ID provided."); return False; try: logging.warning(f"Attempting delete GDrive file ID: {file_id}"); service.files().delete(fileId=file_id).execute(); logging.info(f"Deleted GDrive file ID: {file_id}"); return True; except HttpError as e: if e.resp.status == 404: logging.warning(f"File ID {file_id} not found."); return True; elif e.resp.status == 403: logging.error(f"Permission error deleting {file_id}: {e}"); return False; else: logging.error(f"Google API HTTP error deleting {file_id}: {e}", exc_info=True); return False; except (socket.timeout, requests.exceptions.Timeout, TimeoutError) as e: logging.error(f"Timeout error deleting {file_id}: {e}", exc_info=True); return False; except requests.exceptions.RequestException as e: logging.error(f"Network error deleting {file_id}: {e}", exc_info=True); return False; except Exception as e: logging.critical(f"Unexpected error deleting {file_id}: {e}", exc_info=True); return False

            # --- Kaggle API Setup ---
            # ... (setup_kaggle_api remains unchanged) ...
            KAGGLE_CONFIG_DIR = os.path.expanduser("~/.kaggle"); KAGGLE_JSON_PATH = os.path.join(KAGGLE_CONFIG_DIR, "kaggle.json")
            def setup_kaggle_api(account_index): logging.info(f"Setting up Kaggle API index: {account_index}"); try: try: from main import KAGGLE_CREDENTIALS_LIST; except ImportError: logging.warning("Could not import KAGGLE_CREDENTIALS_LIST. Trying env."); KAGGLE_CREDENTIALS_LIST = [os.environ.get(f'KAGGLE_JSON_{i+1}') for i in range(4)]; if not any(KAGGLE_CREDENTIALS_LIST): logging.critical("Kaggle creds missing from env."); return False; if None in KAGGLE_CREDENTIALS_LIST: logging.warning("Some Kaggle creds missing from env."); logging.info("Loaded KAGGLE_CREDENTIALS_LIST from env."); except Exception as import_e: logging.critical(f"Failed load KAGGLE_CREDENTIALS_LIST: {import_e}"); return False; num_creds = len(KAGGLE_CREDENTIALS_LIST); if not 0 <= account_index < num_creds: logging.error(f"Invalid Kaggle index: {account_index} (of {num_creds})."); return False; kaggle_json_str = KAGGLE_CREDENTIALS_LIST[account_index]; if not kaggle_json_str: logging.error(f"Kaggle creds JSON missing index {account_index}."); return False; try: os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True); with open(KAGGLE_JSON_PATH, 'w') as f: f.write(kaggle_json_str); os.chmod(KAGGLE_JSON_PATH, 0o600); logging.info(f"Kaggle API setup ok index {account_index}."); return True; except (IOError, OSError) as e: logging.critical(f"File I/O error Kaggle setup: {e}", exc_info=True); return False; except Exception as e: logging.critical(f"Unexpected error Kaggle setup: {e}", exc_info=True); return False

            # --- Kaggle Notebook Execution ---
            # ... (trigger_kaggle_notebook, check_kaggle_status, download_kaggle_output remain unchanged) ...
            PARAMS_JSON_FILENAME = "params.json"; PARAMS_DATASET_SLUG = "notebook-params-temp"
            def trigger_kaggle_notebook(notebook_slug, params_dict): logging.info(f"Triggering Kaggle notebook: {notebook_slug}"); try: params_json_str = json.dumps(params_dict); except TypeError as e: logging.error(f"Failed serialize params: {e}"); return False; try: with open(PARAMS_JSON_FILENAME, 'w') as f: f.write(params_json_str); logging.info(f"Created {PARAMS_JSON_FILENAME}"); except (IOError, OSError) as e: logging.error(f"Failed write params file: {e}"); return False; metadata_content = {"title": "Notebook Params Temp", "id": f"{notebook_slug.split('/')[0]}/{PARAMS_DATASET_SLUG}", "licenses": [{"name": "CC0-1.0"}]}; metadata_filename = "dataset-metadata.json"; dataset_created = False; try: with open(metadata_filename, 'w') as f: json.dump(metadata_content, f, indent=4); logging.info(f"Created {metadata_filename}"); logging.info(f"Uploading {PARAMS_JSON_FILENAME} as dataset..."); command = ["kaggle", "datasets", "create", "-p", ".", "-m", "Update params", "--dir-mode", "skip"]; result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=120); if result.stdout: logging.info(f"Kaggle ds create stdout:\n{result.stdout}"); if result.stderr: logging.warning(f"Kaggle ds create stderr:\n{result.stderr}"); if result.returncode != 0 or ("error" in result.stderr.lower() and "error updating dataset" not in result.stderr.lower()): logging.error(f"Kaggle ds create/update failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); dataset_created = False; else: logging.info("Kaggle dataset created/updated."); dataset_created = True; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); dataset_created = False; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle dataset creation."); dataset_created = False; except (IOError, OSError) as e: logging.error(f"File I/O error dataset metadata: {e}"); dataset_created = False; except Exception as e: logging.critical(f"Unexpected error Kaggle dataset creation: {e}", exc_info=True); dataset_created = False; finally: if os.path.exists(PARAMS_JSON_FILENAME): try: os.remove(PARAMS_JSON_FILENAME); except OSError: pass; if os.path.exists(metadata_filename): try: os.remove(metadata_filename); except OSError: pass; if not dataset_created: return False; try: logging.info(f"Triggering Kaggle kernel push: {notebook_slug}"); params_dataset_full_slug = f"{notebook_slug.split('/')[0]}/{PARAMS_DATASET_SLUG}"; dummy_dir = "kaggle_push_dummy"; os.makedirs(dummy_dir, exist_ok=True); kernel_metadata = {"id": notebook_slug, "language": "python", "kernel_type": "notebook", "is_private": "true", "enable_gpu": "true", "enable_internet": "true", "dataset_sources": [params_dataset_full_slug], "competition_sources": [], "kernel_sources": []}; kernel_metadata_path = os.path.join(dummy_dir, "kernel-metadata.json"); with open(kernel_metadata_path, 'w') as f: json.dump(kernel_metadata, f); logging.info(f"Pushing kernel {notebook_slug}..."); command_push = ["kaggle", "kernels", "push", "-p", dummy_dir]; result_push = subprocess.run(command_push, capture_output=True, text=True, check=False, timeout=120); if result_push.stdout: logging.info(f"Kaggle push stdout:\n{result_push.stdout}"); if result_push.stderr: logging.warning(f"Kaggle push stderr:\n{result_push.stderr}"); if result_push.returncode == 0 and "successfully" in result_push.stdout.lower(): logging.info("Kaggle kernel push initiated."); return True; else: logging.error(f"Kaggle push failed/no success msg. Code: {result_push.returncode}."); return False; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return False; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle kernel push."); return False; except (IOError, OSError) as e: logging.error(f"File I/O error kernel push setup: {e}"); return False; except Exception as e: logging.critical(f"Unexpected error Kaggle kernel push: {e}", exc_info=True); return False; finally: if 'dummy_dir' in locals() and os.path.exists(dummy_dir): try: if os.path.exists(kernel_metadata_path): os.remove(kernel_metadata_path); os.rmdir(dummy_dir); except OSError as e: logging.warning(f"Could not cleanup dummy push dir: {e}")
            def check_kaggle_status(notebook_slug): logging.debug(f"Checking Kaggle status: {notebook_slug}"); command = ["kaggle", "kernels", "status", notebook_slug]; try: result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=60); output_line = result.stdout.strip(); if not output_line and result.stderr and "status" in result.stderr.lower(): output_line = result.stderr.strip().split('\n')[-1]; if result.returncode != 0: logging.error(f"Kaggle status cmd failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); if "401" in result.stderr: logging.error("Kaggle API auth error (401)."); elif "404" in result.stderr: logging.error("Kaggle kernel not found (404)."); elif "429" in result.stderr: logging.warning("Kaggle API rate limit (429)."); return None; if not output_line: logging.warning("Kaggle status empty output."); return None; status_part = None; if ":" in output_line: status_part = output_line.split(':')[-1].strip().lower(); elif "-" in output_line: status_part = output_line.split('-')[-1].strip().lower(); if status_part: if "error" in status_part: return "error"; if "complete" in status_part: return "complete"; if "running" in status_part: return "running"; if "cancelled" in status_part: return "cancelled"; if "queued" in status_part: return "queued"; logging.warning(f"Unknown status parsed: '{status_part}'"); return None; else: logging.warning(f"Could not parse status line: '{output_line}'"); return None; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return None; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle status check."); return None; except Exception as e: logging.critical(f"Unexpected error Kaggle status check: {e}", exc_info=True); return None
            def download_kaggle_output(notebook_slug, destination_dir=".", download_image=False): logging.info(f"Attempting download from: {notebook_slug}"); try: os.makedirs(destination_dir, exist_ok=True); except OSError as e: logging.error(f"Failed create dest dir: {e}"); return None, None, None; command = ["kaggle", "kernels", "output", notebook_slug, "-p", destination_dir, "--force"]; mp3_path, json_path, img_path = None, None, None; try: logging.info(f"Running Kaggle download..."); result = subprocess.run(command, capture_output=True, text=True, check=False, timeout=300); if result.stdout: logging.info(f"Kaggle download stdout:\n{result.stdout}"); if result.stderr: logging.warning(f"Kaggle download stderr:\n{result.stderr}"); if result.returncode != 0: stderr_lower = result.stderr.lower(); if "404" in stderr_lower or "not found" in stderr_lower: logging.error("Kaggle download failed: 404."); elif "no output files" in stderr_lower: logging.warning("Kaggle download: No output files."); elif "401" in stderr_lower: logging.error("Kaggle download failed: 401."); elif "429" in stderr_lower: logging.warning("Kaggle download failed: 429."); else: logging.error(f"Kaggle download cmd failed. Code: {result.returncode}. Stderr: {result.stderr.strip()}"); return None, None, None; logging.info("Kaggle download finished. Verifying..."); potential_mp3_path = os.path.join(destination_dir, KAGGLE_OUTPUT_MP3); if os.path.exists(potential_mp3_path) and os.path.getsize(potential_mp3_path) > 100: mp3_path = potential_mp3_path; logging.info(f"Verified MP3: {mp3_path}"); else: logging.warning(f"MP3 '{KAGGLE_OUTPUT_MP3}' missing/empty."); potential_json_path = os.path.join(destination_dir, KAGGLE_OUTPUT_JSON); if os.path.exists(potential_json_path) and os.path.getsize(potential_json_path) > 2: json_path = potential_json_path; logging.info(f"Verified JSON: {json_path}"); else: logging.warning(f"JSON '{KAGGLE_OUTPUT_JSON}' missing/empty."); if download_image: potential_img_path = os.path.join(destination_dir, KAGGLE_OUTPUT_IMG); if os.path.exists(potential_img_path) and os.path.getsize(potential_img_path) > 100: img_path = potential_img_path; logging.info(f"Verified Image: {img_path}"); else: logging.warning(f"Image '{KAGGLE_OUTPUT_IMG}' missing/empty."); return mp3_path, json_path, img_path; except FileNotFoundError as e: logging.critical(f"Kaggle command not found: {e}"); return None, None, None; except subprocess.TimeoutExpired: logging.error("Timeout Kaggle download."); return None, None, None; except Exception as e: logging.critical(f"Unexpected error Kaggle download: {e}", exc_info=True); return None, None, None

            # --- Uniqueness Check ---
            # ... (compare_fingerprints, is_unique_enough remain unchanged) ...
            def compare_fingerprints(fp1_str, fp2_str): if not fp1_str or not fp2_str: return 0.0; return SequenceMatcher(None, fp1_str, fp2_str).ratio()
            def is_unique_enough(new_fingerprint, recent_fingerprints, threshold): if not new_fingerprint: logging.warning("New fingerprint empty."); return False; if not recent_fingerprints: return True; max_similarity = 0.0; for old_fingerprint in recent_fingerprints: similarity = compare_fingerprints(new_fingerprint, old_fingerprint); logging.debug(f"Comparing fingerprints: Sim = {similarity:.4f}"); if similarity > max_similarity: max_similarity = similarity; if similarity > threshold: logging.warning(f"Track too similar (Sim: {similarity:.4f} > Thr: {threshold})."); return False; logging.info(f"Uniqueness check passed. Max sim: {max_similarity:.4f} (Thr: {threshold})"); return True

            # --- Spotify API Interaction ---
            # ... (get_spotify_client, get_spotify_trending_keywords remain unchanged) ...
            _spotify_client = None
            def get_spotify_client(): global _spotify_client; if _spotify_client: return _spotify_client; if not SPOTIPY_AVAILABLE: logging.error("Spotipy not installed."); return None; logging.info("Authenticating with Spotify..."); try: client_id=os.environ.get('SPOTIPY_CLIENT_ID'); client_secret=os.environ.get('SPOTIPY_CLIENT_SECRET'); if not client_id or not client_secret: logging.error("Spotify creds missing."); return None; client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret); sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager); sp.categories(limit=1); logging.info("Spotify client authenticated."); _spotify_client = sp; return _spotify_client; except SpotifyException as e: logging.error(f"Spotify API auth/call error: {e}", exc_info=True); return None; except requests.exceptions.RequestException as e: logging.error(f"Network error Spotify auth: {e}", exc_info=True); return None; except Exception as e: logging.critical(f"Unexpected error Spotify auth: {e}", exc_info=True); return None
            def get_spotify_trending_keywords(limit=10): sp = get_spotify_client(); if not sp: logging.error("Cannot fetch Spotify keywords: client unavailable."); return []; keywords = set(); final_keywords = []; try: logging.info("Fetching Spotify categories..."); categories = sp.categories(country='US', limit=max(limit // 2, 5))['categories']['items']; for category in categories: keywords.add(category['name'].lower()); logging.info("Fetching Spotify new releases..."); new_releases = sp.new_releases(limit=limit)['albums']['items']; for album in new_releases: for artist in album['artists']: try: artist_info = sp.artist(artist['id']); keywords.update(g.lower() for g in artist_info.get('genres', [])); except SpotifyException as art_e: logging.warning(f"Spotify API error artist genres {artist.get('name', artist['id'])}: {art_e}"); except requests.exceptions.RequestException as net_e: logging.warning(f"Network error artist genres {artist.get('name', artist['id'])}: {net_e}"); except Exception as e: logging.warning(f"Unexpected error artist genres {artist.get('name', artist['id'])}: {e}"); final_keywords = list(keywords)[:limit]; logging.info(f"Fetched {len(final_keywords)} Spotify keywords."); except SpotifyException as e: logging.error(f"Spotify API error fetching keywords: {e}", exc_info=True); except requests.exceptions.RequestException as e: logging.error(f"Network error fetching keywords: {e}", exc_info=True); except Exception as e: logging.critical(f"Unexpected error fetching keywords: {e}", exc_info=True); return final_keywords

            # --- Telegram Notification Utility ---
            # ... (send_telegram_message remains unchanged) ...
            _telegram_bot_instance = None
            def _get_telegram_bot(): global _telegram_bot_instance; if _telegram_bot_instance is None: token = os.environ.get('TELEGRAM_BOT_TOKEN'); if not token: logging.error("Cannot init Telegram Bot: Token secret missing."); return None; try: _telegram_bot_instance = telegram.Bot(token=token); logging.info("Telegram Bot instance initialized."); except Exception as e: logging.error(f"Failed init Telegram Bot: {e}", exc_info=True); _telegram_bot_instance = None; return _telegram_bot_instance
            async def _send_message_async(bot, chat_id, text): try: await bot.send_message(chat_id=chat_id, text=text, parse_mode='Markdown'); logging.info(f"Sent Telegram message to chat_id {chat_id}."); return True; except telegram.error.TelegramError as e: logging.error(f"Telegram API error sending message: {e}", exc_info=True); return False; except Exception as e: logging.error(f"Unexpected error sending Telegram message: {e}", exc_info=True); return False
            def send_telegram_message(message, level="INFO"): bot = _get_telegram_bot(); chat_id = os.environ.get('TELEGRAM_CHAT_ID'); if not bot: logging.error("TG message not sent: Bot not initialized."); return False; if not chat_id: logging.error("TG message not sent: Chat ID secret missing."); return False; prefix = f"[{level.upper()}]"; full_message = f"{prefix} {message}"; max_len = 4090; if len(full_message) > max_len: logging.warning(f"TG message too long ({len(full_message)} chars). Truncating."); full_message = full_message[:max_len] + "..."; logging.info(f"Attempting send Telegram message (Level: {level})..."); try: success = asyncio.run(_send_message_async(bot, chat_id, full_message)); return success; except RuntimeError as e: logging.error(f"RuntimeError calling asyncio.run for Telegram: {e}. Loop running?"); return False; except Exception as e: logging.error(f"Unexpected error setting up asyncio for Telegram send: {e}", exc_info=True); return False